{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI, OpenAIError\n",
    "import os\n",
    "import json\n",
    "import configparser\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "client = OpenAI(api_key= config.get('API', 'openai'))\n",
    "\n",
    "# Use sample of size N for training\n",
    "# Random seed 42 used for all sampling \n",
    "N = 300\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(\"biolaysumm2024_data/eLife_val.jsonl\") as f:\n",
    "    for line in f:\n",
    "        lines.append(json.loads(line))\n",
    "\n",
    "train_lines = []\n",
    "with open(\"biolaysumm2024_data/eLife_train.jsonl\") as f:\n",
    "    for line in f:\n",
    "        train_lines.append(json.loads(line))\n",
    "\n",
    "sample = random.sample(train_lines, k=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_summ_file = \"textrank_train_40\"\n",
    "with open(f\"predictions/{textrank_summ_file}.json\") as f:\n",
    "    textrank_preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine \"article\" and lay summary\n",
    "sample_processed = []\n",
    "for i, s in enumerate(sample):\n",
    "    article = s['article'].split(\"\\n\")\n",
    "    text = textrank_preds[i]\n",
    "    summary = s['lay_summary']\n",
    "    sample_processed.append({\n",
    "        'text': text,\n",
    "        'summary': summary\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare in format required for GPT 3.5 fine-tuning\n",
    "with open(\"fine_tune_data_elife_300_textrank40.jsonl\", \"w\") as f:\n",
    "    for example in sample_processed:\n",
    "        obj = {\n",
    "            \"messages\" : [\n",
    "                {\"role\" : \"system\", \"content\" : \"Generate a lay summary of this biomedical article\"},\n",
    "                {\"role\" : \"user\", \"content\" : \"### Article: \\n\" + example['text']},\n",
    "                {\"role\" : \"assistant\", \"content\" : \"### Summary: \\n\" + example['summary']}\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training file\n",
    "client = OpenAI(api_key= config.get('API', 'openai'))\n",
    "client.files.create(\n",
    "  file=open(\"fine_tune_data_elife_200_textrank40.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# Create & run training job\n",
    "client = OpenAI(api_key= config.get('API', 'openai'))\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-numbers\", \n",
    "  model=\"gpt-3.5-turbo-1106\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreieve status of training job\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-XXX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(prompt):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Generate a lay summary of this biomedical article\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens=512\n",
    "    )\n",
    "    pred = chat_completion.choices[0].message.content\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "lines = []\n",
    "with open(\"biolaysumm2024_data/eLife_val.jsonl\") as f:\n",
    "    for line in f:\n",
    "        lines.append(json.loads(line))\n",
    "        \n",
    "with open(\"predictions/textrank_custom_40.json\") as f:\n",
    "    textrank_preds = json.load(f)\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(len(lines))):\n",
    "    text = lines[i]['article'].split(\"\\n\")\n",
    "    prompt = f\"\"\"\\n### Article\\n{textrank_preds[i]}\"\"\"\n",
    "    predictions.append(completion_with_backoff(prompt))\n",
    "\n",
    "new_preds = []\n",
    "for p in predictions:\n",
    "    p = p.replace(\"### Summary\", \"\")\n",
    "    p = p.replace(\"*\", \"\")\n",
    "    new_preds.append(p.strip())\n",
    "predictions = new_preds\n",
    "\n",
    "with open(\"predictions_gpt35_elife_textrank_40.json\", \"w\") as f:\n",
    "    json.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
